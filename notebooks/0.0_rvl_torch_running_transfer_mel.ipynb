{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "62fd4cae-dac7-4f2f-9d91-39bf2408e78c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c8bd079e-9875-47ba-9b82-06a447aa9284",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frozen parameter: features.0.weight, requires_grad: False\n",
      "Frozen parameter: features.0.bias, requires_grad: False\n",
      "Frozen parameter: features.3.weight, requires_grad: False\n",
      "Frozen parameter: features.3.bias, requires_grad: False\n",
      "Frozen parameter: features.6.weight, requires_grad: False\n",
      "Frozen parameter: features.6.bias, requires_grad: False\n",
      "Frozen parameter: features.8.weight, requires_grad: False\n",
      "Frozen parameter: features.8.bias, requires_grad: False\n",
      "Frozen parameter: features.11.weight, requires_grad: False\n",
      "Frozen parameter: features.11.bias, requires_grad: False\n",
      "Frozen parameter: features.13.weight, requires_grad: False\n",
      "Frozen parameter: features.13.bias, requires_grad: False\n",
      "Frozen parameter: embeddings.0.weight, requires_grad: False\n",
      "Frozen parameter: embeddings.0.bias, requires_grad: False\n",
      "Unfrozen parameter: embeddings.2.weight, requires_grad: True\n",
      "Unfrozen parameter: embeddings.2.bias, requires_grad: True\n",
      "Unfrozen parameter: embeddings.4.weight, requires_grad: True\n",
      "Unfrozen parameter: embeddings.4.bias, requires_grad: True\n",
      "Total trainable parameters: 17314049\n",
      "\n",
      "Epoch 1/1\n",
      "Processing batch 1\n",
      "Processing batch 2\n",
      "Processing batch 3\n",
      "Batch 3, Loss: 22.3776\n",
      "Processing batch 4\n",
      "Processing batch 5\n",
      "Processing batch 6\n",
      "Batch 6, Loss: 12.7700\n",
      "Processing batch 7\n",
      "Processing batch 8\n",
      "Processing batch 9\n",
      "Batch 9, Loss: 7.5914\n",
      "Processing batch 10\n",
      "Processing batch 11\n",
      "Processing batch 12\n",
      "Batch 12, Loss: 4.0018\n",
      "Processing batch 13\n",
      "Processing batch 14\n",
      "Processing batch 15\n",
      "Batch 15, Loss: 6.9550\n",
      "Processing batch 16\n",
      "Processing batch 17\n",
      "Processing batch 18\n",
      "Batch 18, Loss: 4.8321\n",
      "Processing batch 19\n",
      "Processing batch 20\n",
      "Processing batch 21\n",
      "Batch 21, Loss: 5.1763\n",
      "Processing batch 22\n",
      "Processing batch 23\n",
      "Processing batch 24\n",
      "Batch 24, Loss: 4.2952\n",
      "Processing batch 25\n",
      "Processing batch 26\n",
      "Processing batch 27\n",
      "Batch 27, Loss: 2.8100\n",
      "Processing batch 28\n",
      "Processing batch 29\n",
      "Processing batch 30\n",
      "Batch 30, Loss: 2.3517\n",
      "Processing batch 31\n",
      "Processing batch 32\n",
      "Processing batch 33\n",
      "Batch 33, Loss: 3.1713\n",
      "Processing batch 34\n",
      "Processing batch 35\n",
      "Processing batch 36\n",
      "Batch 36, Loss: 1.9666\n",
      "Processing batch 37\n",
      "Processing batch 38\n",
      "Processing batch 39\n",
      "Batch 39, Loss: 3.0519\n",
      "Processing batch 40\n",
      "Processing batch 41\n",
      "Processing batch 42\n",
      "Batch 42, Loss: 2.1838\n",
      "Processing batch 43\n",
      "Processing batch 44\n",
      "Processing batch 45\n",
      "Batch 45, Loss: 2.0290\n",
      "Processing batch 46\n",
      "Processing batch 47\n",
      "Processing batch 48\n",
      "Batch 48, Loss: 1.9008\n",
      "Processing batch 49\n",
      "Processing batch 50\n",
      "Processing batch 51\n",
      "Batch 51, Loss: 2.0063\n",
      "Processing batch 52\n",
      "Processing batch 53\n",
      "Processing batch 54\n",
      "Batch 54, Loss: 1.6105\n",
      "Processing batch 55\n",
      "Processing batch 56\n",
      "Processing batch 57\n",
      "Batch 57, Loss: 1.3844\n",
      "Processing batch 58\n",
      "Processing batch 59\n",
      "Processing batch 60\n",
      "Batch 60, Loss: 3.9506\n",
      "Processing batch 61\n",
      "Processing batch 62\n",
      "\n",
      "Test Results:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       Minor       0.44      0.14      0.21       120\n",
      "       Major       0.57      0.86      0.69       160\n",
      "\n",
      "    accuracy                           0.55       280\n",
      "   macro avg       0.50      0.50      0.45       280\n",
      "weighted avg       0.51      0.55      0.48       280\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchaudio\n",
    "import torchvggish\n",
    "import os\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torchaudio.transforms as T\n",
    "\n",
    "class ChordDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, file_paths, labels, sample_rate=16000):\n",
    "        self.file_paths = file_paths\n",
    "        self.labels = labels\n",
    "        self.sample_rate = sample_rate\n",
    "        \n",
    "        # VGGish expects 96 mel bands\n",
    "        self.mel_transform = T.MelSpectrogram(\n",
    "            sample_rate=sample_rate,\n",
    "            n_fft=2048,\n",
    "            win_length=400,\n",
    "            hop_length=160,\n",
    "            n_mels=96,\n",
    "            f_min=125,\n",
    "            f_max=7500\n",
    "        )\n",
    "        \n",
    "        # Log mel spectrogram\n",
    "        self.amplitude_to_db = T.AmplitudeToDB()\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.file_paths)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Load audio\n",
    "        waveform, sr = torchaudio.load(self.file_paths[idx])\n",
    "        #print(f\"Getting: {self.file_paths[idx]}\")\n",
    "        \n",
    "        # Resample if necessary\n",
    "        if sr != self.sample_rate:\n",
    "            resampler = T.Resample(sr, self.sample_rate)\n",
    "            waveform = resampler(waveform)\n",
    "        \n",
    "        # Convert to mono if stereo\n",
    "        if waveform.size(0) > 1:\n",
    "            waveform = torch.mean(waveform, dim=0, keepdim=True)\n",
    "        \n",
    "        # Get mel spectrogram\n",
    "        mel_spec = self.mel_transform(waveform)\n",
    "        \n",
    "        # Convert to dB scale\n",
    "        mel_spec = self.amplitude_to_db(mel_spec)\n",
    "        #print(f\"Getting mel_spec: {mel_spec}\")\n",
    "        \n",
    "        # VGGish expects input size of (batch_size, 1, 96, 64)\n",
    "        # So we need to ensure our time dimension is 64 frames\n",
    "        target_length = 64\n",
    "        current_length = mel_spec.size(2)\n",
    "        \n",
    "        if current_length < target_length:\n",
    "            # Pad if too short\n",
    "            padding = target_length - current_length\n",
    "            mel_spec = torch.nn.functional.pad(mel_spec, (0, padding))\n",
    "            \n",
    "        elif current_length > target_length:\n",
    "            # Take center portion if too long\n",
    "            start = (current_length - target_length) // 2\n",
    "            mel_spec = mel_spec[:, :, start:start + target_length]\n",
    "        \n",
    "        # Add channel dimension\n",
    "       # mel_spec = mel_spec.unsqueeze(0)\n",
    "            \n",
    "        return mel_spec, torch.tensor(self.labels[idx], dtype=torch.float32)\n",
    "\n",
    "class ChordClassifier(nn.Module):\n",
    "    def __init__(self, num_classes=1):\n",
    "        super().__init__()\n",
    "        self.feature_extractor = torchvggish.vggish()\n",
    "\n",
    "        # Freeze layers, but keep last layers unfrozen\n",
    "        for name, param in self.feature_extractor.named_parameters():\n",
    "            if 'embeddings.4' in name or 'embeddings.2' in name:# or 'embeddings.0' in name:  # Unfreeze only the last layer\n",
    "                param.requires_grad = True\n",
    "                print(f\"Unfrozen parameter: {name}, requires_grad: {param.requires_grad}\")\n",
    "            else:\n",
    "                param.requires_grad = False  # Keep all other layers frozen\n",
    "                print(f\"Frozen parameter: {name}, requires_grad: {param.requires_grad}\")\n",
    "        \n",
    "            \n",
    "        # Simple classifier on top of VGGish embeddings\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(128, 64),  # VGGish outputs 128-dimensional embeddings\n",
    "            nn.ReLU(),\n",
    "            #nn.Dropout(0.5),\n",
    "            nn.Linear(64, num_classes),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Remove torch.no_grad() here\n",
    "        features = self.feature_extractor(x)  # Remove torch.no_grad() here\n",
    "        return self.classifier(features)\n",
    "\n",
    "\n",
    "class ChordTrainer:\n",
    "    def __init__(self, model, criterion, optimizer, device, threshold=0.5):\n",
    "        self.model = model\n",
    "        self.criterion = criterion\n",
    "        self.optimizer = optimizer\n",
    "        self.device = device\n",
    "        self.threshold = threshold\n",
    "        \n",
    "    def train_epoch(self, dataloader):\n",
    "\n",
    "        self.model.train()\n",
    "        running_loss = 0.0\n",
    "\n",
    "            # Verify parameters are unfrozen\n",
    "        for param in self.model.parameters():\n",
    "            param.requires_grad = True\n",
    "        \n",
    "        for i, (inputs, labels) in enumerate(dataloader):\n",
    "            print(\"Processing batch\", i+1)\n",
    "            \n",
    "            inputs, labels = inputs.to(self.device), labels.to(self.device)\n",
    "            \n",
    "            self.optimizer.zero_grad()\n",
    "                      \n",
    "            outputs = self.model(inputs)\n",
    "            loss = self.criterion(outputs.squeeze(), labels)\n",
    "\n",
    "            loss.backward()\n",
    "            self.optimizer.step()           \n",
    "            running_loss += loss.item()\n",
    "            \n",
    "            if (i + 1) % 3 == 0:\n",
    "                avg_loss = running_loss / 3\n",
    "                print(f'Batch {i+1}, Loss: {avg_loss:.4f}')\n",
    "                running_loss = 0.0\n",
    "\n",
    "    \n",
    "                \n",
    "    def evaluate(self, dataloader):\n",
    "        self.model.eval()\n",
    "        all_preds = []\n",
    "        all_labels = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in dataloader:\n",
    "                inputs, labels = inputs.to(self.device), labels.to(self.device)\n",
    "                outputs = self.model(inputs)\n",
    "                predicted = (outputs.squeeze() > self.threshold).float()\n",
    "                all_preds.extend(predicted.cpu().numpy())\n",
    "                all_labels.extend(labels.cpu().numpy())\n",
    "        \n",
    "        return classification_report(\n",
    "            all_labels, \n",
    "            all_preds, \n",
    "            target_names=[\"Minor\", \"Major\"], \n",
    "            zero_division=0\n",
    "        )\n",
    "\n",
    "def get_dataloader(file_paths, labels, batch_size=16, shuffle=True):\n",
    "    def worker_init_fn(worker_id):\n",
    "        np.random.seed(42 + worker_id)\n",
    "    \n",
    "    dataset = ChordDataset(file_paths, labels)\n",
    "    return DataLoader(dataset, batch_size=batch_size, shuffle=shuffle, worker_init_fn=worker_init_fn)\n",
    "\n",
    "\n",
    "\n",
    "def main():\n",
    "\n",
    "    set_seed(42)\n",
    "    \n",
    "    # Setup\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    file_dir = r'C:\\Users\\rapha\\repositories\\guitar_hero\\data\\raw\\kaggle_chords\\Training'\n",
    "    file_dir_test = r'C:\\Users\\rapha\\repositories\\guitar_hero\\data\\raw\\kaggle_chords\\Test'\n",
    "    \n",
    "        # Function to gather all .wav file paths from subdirectories\n",
    "    def gather_file_paths(dir_path):\n",
    "        file_paths = []\n",
    "        for root, dirs, files in os.walk(dir_path):\n",
    "            \n",
    "            # Gather all .wav files\n",
    "            wav_files = [os.path.join(root, f) for f in files if f.endswith('.wav')]\n",
    "            file_paths.extend(wav_files)\n",
    "            \n",
    "        return file_paths\n",
    "    \n",
    "    # Gather all file paths from Training and Test directories\n",
    "    file_paths = gather_file_paths(file_dir)\n",
    "    file_paths_test = gather_file_paths(file_dir_test)\n",
    "    \n",
    "    labels = [0 if 'Minor' in f else 1 for f in file_paths if f.endswith('.wav')]\n",
    "    test_labels = [0 if 'Minor' in f else 1 for f in file_paths_test if f.endswith('.wav')]\n",
    "    \n",
    "    #train_paths, test_paths, train_labels, test_labels = train_test_split(\n",
    "    #    file_paths, labels, test_size=0.2, random_state=42, stratify=labels\n",
    "    #)\n",
    "\n",
    "    #print(labels)\n",
    "\n",
    "    #Changed for train set evaluation!\n",
    "    train_dataloader = get_dataloader(file_paths, labels, batch_size=16)\n",
    "    test_dataloader = get_dataloader(file_paths_test, test_labels, batch_size=16)\n",
    "    \n",
    "        # Initialize model\n",
    "    model = ChordClassifier().to(device)\n",
    "    \n",
    "    # Check trainable parameters\n",
    "    total_trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    print(f\"Total trainable parameters: {total_trainable_params}\")\n",
    "    \n",
    "    # Training setup\n",
    "    criterion = nn.BCELoss()\n",
    "\n",
    "    \n",
    "    #optimizer = torch.optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
    "    trainer = ChordTrainer(model, criterion, optimizer, device, threshold=0.3)\n",
    "    \n",
    "    # Training loop\n",
    "    num_epochs = 1\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f'\\nEpoch {epoch + 1}/{num_epochs}')\n",
    "        trainer.train_epoch(train_dataloader)\n",
    "        \n",
    "    # Evaluation\n",
    "    report = trainer.evaluate(test_dataloader)\n",
    "    print('\\nTest Results:')\n",
    "    print(report)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # Set the seed value\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7ef312df-6bc0-436b-b7db-2eafbdc375c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torchvggish\n",
      "  Using cached torchvggish-0.2-py3-none-any.whl\n",
      "Requirement already satisfied: numpy in c:\\users\\rapha\\repositories\\guitar_hero\\.envs\\dev_env\\lib\\site-packages (from torchvggish) (2.0.2)\n",
      "Requirement already satisfied: torch in c:\\users\\rapha\\repositories\\guitar_hero\\.envs\\dev_env\\lib\\site-packages (from torchvggish) (2.4.1)\n",
      "Collecting resampy (from torchvggish)\n",
      "  Using cached resampy-0.4.3-py3-none-any.whl.metadata (3.0 kB)\n",
      "Requirement already satisfied: numba>=0.53 in c:\\users\\rapha\\repositories\\guitar_hero\\.envs\\dev_env\\lib\\site-packages (from resampy->torchvggish) (0.60.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\rapha\\repositories\\guitar_hero\\.envs\\dev_env\\lib\\site-packages (from torch->torchvggish) (3.16.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\rapha\\repositories\\guitar_hero\\.envs\\dev_env\\lib\\site-packages (from torch->torchvggish) (4.12.2)\n",
      "Requirement already satisfied: sympy in c:\\users\\rapha\\repositories\\guitar_hero\\.envs\\dev_env\\lib\\site-packages (from torch->torchvggish) (1.13.3)\n",
      "Requirement already satisfied: networkx in c:\\users\\rapha\\repositories\\guitar_hero\\.envs\\dev_env\\lib\\site-packages (from torch->torchvggish) (3.4.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\rapha\\repositories\\guitar_hero\\.envs\\dev_env\\lib\\site-packages (from torch->torchvggish) (3.1.4)\n",
      "Requirement already satisfied: fsspec in c:\\users\\rapha\\repositories\\guitar_hero\\.envs\\dev_env\\lib\\site-packages (from torch->torchvggish) (2024.9.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\rapha\\repositories\\guitar_hero\\.envs\\dev_env\\lib\\site-packages (from torch->torchvggish) (75.1.0)\n",
      "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in c:\\users\\rapha\\repositories\\guitar_hero\\.envs\\dev_env\\lib\\site-packages (from numba>=0.53->resampy->torchvggish) (0.43.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\rapha\\repositories\\guitar_hero\\.envs\\dev_env\\lib\\site-packages (from jinja2->torch->torchvggish) (3.0.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\rapha\\repositories\\guitar_hero\\.envs\\dev_env\\lib\\site-packages (from sympy->torch->torchvggish) (1.3.0)\n",
      "Using cached resampy-0.4.3-py3-none-any.whl (3.1 MB)\n",
      "Installing collected packages: resampy, torchvggish\n",
      "Successfully installed resampy-0.4.3 torchvggish-0.2\n"
     ]
    }
   ],
   "source": [
    "!pip install torchvggish"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5eedc1be-d209-462d-8103-ea9017bd2afe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All filenames in 'C:\\Users\\rapha\\repositories\\guitar_hero\\data\\raw\\kaggle_chords\\Test\\Em' updated successfully.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "def add_minor_to_filenames(directory_path):\n",
    "    \"\"\"\n",
    "    This function adds '-Minor' to all filenames in the specified directory.\n",
    "    \n",
    "    :param directory_path: Path to the directory where files are located\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Loop through all files in the specified directory\n",
    "        for filename in os.listdir(directory_path):\n",
    "            # Construct full file path\n",
    "            full_file_path = os.path.join(directory_path, filename)\n",
    "            \n",
    "            # Check if it is a file (not a directory)\n",
    "            if os.path.isfile(full_file_path):\n",
    "                # Add '-Minor' before the file extension\n",
    "                name, ext = os.path.splitext(filename)\n",
    "                new_name = f\"{name}-Minor{ext}\"\n",
    "                \n",
    "                # Construct full new file path\n",
    "                new_file_path = os.path.join(directory_path, new_name)\n",
    "                \n",
    "                # Rename the file\n",
    "                os.rename(full_file_path, new_file_path)\n",
    "                \n",
    "        print(f\"All filenames in '{directory_path}' updated successfully.\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "\n",
    "# Example usage:\n",
    "add_minor_to_filenames(r\"C:\\Users\\rapha\\repositories\\guitar_hero\\data\\raw\\kaggle_chords\\Test\\Em\")  # Replace with your directory path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d93c8edf-650e-4e3a-9103-5a3a724bc78e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "New guitar her env",
   "language": "python",
   "name": "guitar_hero"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
