{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "62fd4cae-dac7-4f2f-9d91-39bf2408e78c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c8bd079e-9875-47ba-9b82-06a447aa9284",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "280\n",
      "Frozen parameter: features.0.weight, requires_grad: False\n",
      "Frozen parameter: features.0.bias, requires_grad: False\n",
      "Frozen parameter: features.3.weight, requires_grad: False\n",
      "Frozen parameter: features.3.bias, requires_grad: False\n",
      "Frozen parameter: features.6.weight, requires_grad: False\n",
      "Frozen parameter: features.6.bias, requires_grad: False\n",
      "Frozen parameter: features.8.weight, requires_grad: False\n",
      "Frozen parameter: features.8.bias, requires_grad: False\n",
      "Frozen parameter: features.11.weight, requires_grad: False\n",
      "Frozen parameter: features.11.bias, requires_grad: False\n",
      "Frozen parameter: features.13.weight, requires_grad: False\n",
      "Frozen parameter: features.13.bias, requires_grad: False\n",
      "Unfrozen parameter: embeddings.0.weight, requires_grad: False\n",
      "Unfrozen parameter: embeddings.0.bias, requires_grad: False\n",
      "Unfrozen parameter: embeddings.2.weight, requires_grad: False\n",
      "Unfrozen parameter: embeddings.2.bias, requires_grad: False\n",
      "Unfrozen parameter: embeddings.4.weight, requires_grad: False\n",
      "Unfrozen parameter: embeddings.4.bias, requires_grad: False\n",
      "Total trainable parameters: 8321\n",
      "\n",
      "Epoch 1/1\n",
      "Processing batch 1\n",
      "Processing batch 2\n",
      "Processing batch 3\n",
      "Batch 3, Training Loss: 17.5660\n",
      "Batch 3, Validation Loss: 9.7381\n",
      "Processing batch 4\n",
      "Processing batch 5\n",
      "Processing batch 6\n",
      "Batch 6, Training Loss: 7.7024\n",
      "Batch 6, Validation Loss: 4.9705\n",
      "Processing batch 7\n",
      "Processing batch 8\n",
      "Processing batch 9\n",
      "Batch 9, Training Loss: 3.6848\n",
      "Batch 9, Validation Loss: 1.5189\n",
      "Processing batch 10\n",
      "Processing batch 11\n",
      "Processing batch 12\n",
      "Batch 12, Training Loss: 1.7095\n",
      "Batch 12, Validation Loss: 2.8283\n",
      "Processing batch 13\n",
      "Processing batch 14\n",
      "Processing batch 15\n",
      "Batch 15, Training Loss: 3.3410\n",
      "Batch 15, Validation Loss: 3.1417\n",
      "Processing batch 16\n",
      "Processing batch 17\n",
      "Processing batch 18\n",
      "Batch 18, Training Loss: 2.1468\n",
      "Batch 18, Validation Loss: 2.4025\n",
      "Processing batch 19\n",
      "Processing batch 20\n",
      "Processing batch 21\n",
      "Batch 21, Training Loss: 1.2271\n",
      "Batch 21, Validation Loss: 1.4279\n",
      "Processing batch 22\n",
      "Processing batch 23\n",
      "Processing batch 24\n",
      "Batch 24, Training Loss: 1.4301\n",
      "Batch 24, Validation Loss: 1.4289\n",
      "Processing batch 25\n",
      "Processing batch 26\n",
      "Processing batch 27\n",
      "Batch 27, Training Loss: 1.6005\n",
      "Batch 27, Validation Loss: 1.5062\n",
      "Processing batch 28\n",
      "Processing batch 29\n",
      "Processing batch 30\n",
      "Batch 30, Training Loss: 1.3324\n",
      "Batch 30, Validation Loss: 1.3802\n",
      "Processing batch 31\n",
      "Processing batch 32\n",
      "Processing batch 33\n",
      "Batch 33, Training Loss: 1.6458\n",
      "Batch 33, Validation Loss: 1.3415\n",
      "Processing batch 34\n",
      "Processing batch 35\n",
      "Processing batch 36\n",
      "Batch 36, Training Loss: 1.1658\n",
      "Batch 36, Validation Loss: 1.2727\n",
      "Processing batch 37\n",
      "Processing batch 38\n",
      "Processing batch 39\n",
      "Batch 39, Training Loss: 1.2902\n",
      "Batch 39, Validation Loss: 1.3257\n",
      "Processing batch 40\n",
      "Processing batch 41\n",
      "Processing batch 42\n",
      "Batch 42, Training Loss: 1.3101\n",
      "Batch 42, Validation Loss: 1.4770\n",
      "Processing batch 43\n",
      "Processing batch 44\n",
      "Processing batch 45\n",
      "Batch 45, Training Loss: 1.5581\n",
      "Batch 45, Validation Loss: 1.1797\n",
      "Processing batch 46\n",
      "Processing batch 47\n",
      "Processing batch 48\n",
      "Batch 48, Training Loss: 1.1017\n",
      "Batch 48, Validation Loss: 1.2633\n",
      "Processing batch 49\n",
      "\n",
      "Validation Results:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       Minor       0.40      0.21      0.28        84\n",
      "       Major       0.56      0.76      0.65       112\n",
      "\n",
      "    accuracy                           0.53       196\n",
      "   macro avg       0.48      0.49      0.46       196\n",
      "weighted avg       0.49      0.53      0.49       196\n",
      "\n",
      "\n",
      "Test Results:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       Minor       0.44      0.15      0.22       120\n",
      "       Major       0.57      0.86      0.69       160\n",
      "\n",
      "    accuracy                           0.55       280\n",
      "   macro avg       0.51      0.50      0.46       280\n",
      "weighted avg       0.52      0.55      0.49       280\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchaudio\n",
    "import torchvggish\n",
    "import os\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torchaudio.transforms as T\n",
    "\n",
    "class ChordDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, file_paths, labels, sample_rate=16000):\n",
    "        self.file_paths = file_paths\n",
    "        self.labels = labels\n",
    "        self.sample_rate = sample_rate\n",
    "        \n",
    "        # VGGish expects 96 mel bands\n",
    "        self.mel_transform = T.MelSpectrogram(\n",
    "            sample_rate=sample_rate,\n",
    "            n_fft=2048,\n",
    "            win_length=400,\n",
    "            hop_length=160,\n",
    "            n_mels=96,\n",
    "            f_min=125,\n",
    "            f_max=7500\n",
    "        )\n",
    "        \n",
    "        # Log mel spectrogram\n",
    "        self.amplitude_to_db = T.AmplitudeToDB()\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.file_paths)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Load audio\n",
    "        waveform, sr = torchaudio.load(self.file_paths[idx])\n",
    "        #print(f\"Getting: {self.file_paths[idx]}\")\n",
    "        \n",
    "        # Resample if necessary\n",
    "        if sr != self.sample_rate:\n",
    "            resampler = T.Resample(sr, self.sample_rate)\n",
    "            waveform = resampler(waveform)\n",
    "        \n",
    "        # Convert to mono if stereo\n",
    "        if waveform.size(0) > 1:\n",
    "            waveform = torch.mean(waveform, dim=0, keepdim=True)\n",
    "        \n",
    "        # Get mel spectrogram\n",
    "        mel_spec = self.mel_transform(waveform)\n",
    "        \n",
    "        # Convert to dB scale\n",
    "        mel_spec = self.amplitude_to_db(mel_spec)\n",
    "\n",
    "        # Add normalization step for mel spectrograms\n",
    "        mel_spec = (mel_spec - mel_spec.mean()) / (mel_spec.std() + 1e-8)\n",
    "        #print(f\"Getting mel_spec: {mel_spec}\")\n",
    "        \n",
    "        # VGGish expects input size of (batch_size, 1, 96, 64)\n",
    "        # So we need to ensure our time dimension is 64 frames\n",
    "        target_length = 64\n",
    "        current_length = mel_spec.size(2)\n",
    "        \n",
    "        if current_length < target_length:\n",
    "            # Pad if too short\n",
    "            padding = target_length - current_length\n",
    "            mel_spec = torch.nn.functional.pad(mel_spec, (0, padding))\n",
    "            \n",
    "        elif current_length > target_length:\n",
    "            # Take center portion if too long\n",
    "            start = (current_length - target_length) // 2\n",
    "            mel_spec = mel_spec[:, :, start:start + target_length]\n",
    "        \n",
    "        # Add channel dimension\n",
    "       # mel_spec = mel_spec.unsqueeze(0)\n",
    "            \n",
    "        return mel_spec, torch.tensor(self.labels[idx], dtype=torch.float32)\n",
    "\n",
    "class ChordClassifier(nn.Module):\n",
    "    def __init__(self, num_classes=1):\n",
    "        super().__init__()\n",
    "        self.feature_extractor = torchvggish.vggish()\n",
    "\n",
    "        # Freeze layers, but keep last layers unfrozen\n",
    "        for name, param in self.feature_extractor.named_parameters():\n",
    "            if 'embeddings.4' in name or 'embeddings.2' in name or 'embeddings.0' in name:  # Unfreeze only the last layer\n",
    "                param.requires_grad = False\n",
    "                print(f\"Unfrozen parameter: {name}, requires_grad: {param.requires_grad}\")\n",
    "            else:\n",
    "                param.requires_grad = False  # Keep all other layers frozen\n",
    "                print(f\"Frozen parameter: {name}, requires_grad: {param.requires_grad}\")\n",
    "        \n",
    "            \n",
    "        # Simple classifier on top of VGGish embeddings\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(128, 64),  # VGGish outputs 128-dimensional embeddings\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(64, num_classes),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Remove torch.no_grad() here\n",
    "        features = self.feature_extractor(x)  # Remove torch.no_grad() here\n",
    "        return self.classifier(features)\n",
    "\n",
    "class ChordTrainer:\n",
    "    def __init__(self, model, criterion, optimizer, device, threshold=0.5):\n",
    "        self.model = model\n",
    "        self.criterion = criterion\n",
    "        self.optimizer = optimizer\n",
    "        self.device = device\n",
    "        self.threshold = threshold\n",
    "        \n",
    "    def train_epoch(self, train_dataloader, val_dataloader):\n",
    "        self.model.train()\n",
    "        running_loss = 0.0\n",
    "\n",
    "        for i, (inputs, labels) in enumerate(train_dataloader):\n",
    "            print(\"Processing batch\", i + 1)\n",
    "            \n",
    "            inputs, labels = inputs.to(self.device), labels.to(self.device)\n",
    "              \n",
    "            outputs = self.model(inputs)\n",
    "            loss = self.criterion(outputs.squeeze(), labels)\n",
    "\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            self.optimizer.step()           \n",
    "            running_loss += loss.item()\n",
    "            \n",
    "            # Every 3 batches, compute average training loss and validation loss\n",
    "            if (i + 1) % 3 == 0:\n",
    "                avg_train_loss = running_loss / 3\n",
    "                print(f'Batch {i + 1}, Training Loss: {avg_train_loss:.4f}')\n",
    "                running_loss = 0.0\n",
    "\n",
    "                # Compute validation loss\n",
    "                val_loss = self.evaluate_loss(val_dataloader)\n",
    "                print(f'Batch {i + 1}, Validation Loss: {val_loss:.4f}')\n",
    "\n",
    "    def evaluate_loss(self, dataloader):\n",
    "        self.model.eval()\n",
    "        running_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in dataloader:\n",
    "                inputs, labels = inputs.to(self.device), labels.to(self.device)\n",
    "                outputs = self.model(inputs)\n",
    "                loss = self.criterion(outputs.squeeze(), labels)\n",
    "                running_loss += loss.item()\n",
    "\n",
    "        # Return average loss over the validation set\n",
    "        return running_loss / len(dataloader)\n",
    "\n",
    "    def evaluate(self, dataloader):\n",
    "        self.model.eval()\n",
    "        all_preds = []\n",
    "        all_labels = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in dataloader:\n",
    "                inputs, labels = inputs.to(self.device), labels.to(self.device)\n",
    "                outputs = self.model(inputs)\n",
    "                predicted = (outputs.squeeze() > self.threshold).float()\n",
    "                all_preds.extend(predicted.cpu().numpy())\n",
    "                all_labels.extend(labels.cpu().numpy())\n",
    "        \n",
    "        return classification_report(\n",
    "            all_labels, \n",
    "            all_preds, \n",
    "            target_names=[\"Minor\", \"Major\"], \n",
    "            zero_division=0\n",
    "        )\n",
    "\n",
    "    \n",
    "\n",
    "def get_dataloader(file_paths, labels, batch_size=16, shuffle=True):\n",
    "    def worker_init_fn(worker_id):\n",
    "        np.random.seed(42 + worker_id)\n",
    "    \n",
    "    dataset = ChordDataset(file_paths, labels)\n",
    "    return DataLoader(dataset, batch_size=batch_size, shuffle=shuffle, worker_init_fn=worker_init_fn)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def main():\n",
    "\n",
    "    set_seed(42)\n",
    "    \n",
    "    # Setup\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    file_dir = r'C:\\Users\\rapha\\repositories\\guitar_vision\\data\\raw\\kaggle_chords\\Training'\n",
    "    file_dir_test = r'C:\\Users\\rapha\\repositories\\guitar_vision\\data\\raw\\kaggle_chords\\Testing'\n",
    "    #file_dir_test = r'C:\\Users\\rapha\\repositories\\guitar_vision\\data\\raw\\other'\n",
    "    \n",
    "    # Function to gather all .wav file paths from subdirectories\n",
    "    def gather_file_paths(dir_path):\n",
    "        file_paths = []\n",
    "        for root, dirs, files in os.walk(dir_path):\n",
    "            wav_files = [os.path.join(root, f) for f in files if f.endswith('.wav')]\n",
    "            file_paths.extend(wav_files)\n",
    "        return file_paths\n",
    "\n",
    "    # Gather all file paths from the directory\n",
    "    file_paths = gather_file_paths(file_dir)\n",
    "    file_paths_test = gather_file_paths(file_dir_test)\n",
    "\n",
    "    print(len(file_paths_test))\n",
    "    \n",
    "    # Create labels\n",
    "    labels = [0 if 'Minor' in f else 1 for f in file_paths if f.endswith('.wav')]\n",
    "    test_labels = [0 if 'Minor' in f else 1 for f in file_paths_test if f.endswith('.wav')]\n",
    "\n",
    "    # Train-validation split (80% train, 20% validation)\n",
    "    train_paths, val_paths, train_labels, val_labels = train_test_split(\n",
    "        file_paths, labels, test_size=0.2, random_state=42, stratify=labels\n",
    "    )\n",
    "\n",
    "    batch_size = 16\n",
    "    \n",
    "    # DataLoader for training and validation sets\n",
    "    train_dataloader = get_dataloader(train_paths, train_labels, batch_size=batch_size)\n",
    "    val_dataloader = get_dataloader(val_paths, val_labels, batch_size=batch_size)\n",
    "    test_dataloader = get_dataloader(file_paths_test, test_labels, batch_size=batch_size)\n",
    "    \n",
    "    # Initialize model\n",
    "    model = ChordClassifier().to(device)\n",
    "    \n",
    "    # Check trainable parameters\n",
    "    total_trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    print(f\"Total trainable parameters: {total_trainable_params}\")\n",
    "    \n",
    "    # Training setup\n",
    "    criterion = nn.BCELoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
    "    trainer = ChordTrainer(model, criterion, optimizer, device, threshold=0.5)\n",
    "    \n",
    "    # Training loop\n",
    "    num_epochs = 1\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f'\\nEpoch {epoch + 1}/{num_epochs}')\n",
    "        trainer.train_epoch(train_dataloader, val_dataloader)  # Pass both train and val dataloaders\n",
    "\n",
    "    # Evaluation on validation set\n",
    "    report = trainer.evaluate(val_dataloader)\n",
    "    print('\\nValidation Results:')\n",
    "    print(report)\n",
    "\n",
    "     # Evaluation\n",
    "    report = trainer.evaluate(test_dataloader)\n",
    "    print('\\nTest Results:')\n",
    "    print(report)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # Set the seed value\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5eedc1be-d209-462d-8103-ea9017bd2afe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All filenames in 'C:\\Users\\rapha\\repositories\\guitar_hero\\data\\raw\\kaggle_chords\\Test\\Em' updated successfully.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "def add_minor_to_filenames(directory_path):\n",
    "    \"\"\"\n",
    "    This function adds '-Minor' to all filenames in the specified directory.\n",
    "    \n",
    "    :param directory_path: Path to the directory where files are located\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Loop through all files in the specified directory\n",
    "        for filename in os.listdir(directory_path):\n",
    "            # Construct full file path\n",
    "            full_file_path = os.path.join(directory_path, filename)\n",
    "            \n",
    "            # Check if it is a file (not a directory)\n",
    "            if os.path.isfile(full_file_path):\n",
    "                # Add '-Minor' before the file extension\n",
    "                name, ext = os.path.splitext(filename)\n",
    "                new_name = f\"{name}-Minor{ext}\"\n",
    "                \n",
    "                # Construct full new file path\n",
    "                new_file_path = os.path.join(directory_path, new_name)\n",
    "                \n",
    "                # Rename the file\n",
    "                os.rename(full_file_path, new_file_path)\n",
    "                \n",
    "        print(f\"All filenames in '{directory_path}' updated successfully.\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "\n",
    "# Example usage:\n",
    "add_minor_to_filenames(r\"C:\\Users\\rapha\\repositories\\guitar_hero\\data\\raw\\kaggle_chords\\Test\\Em\")  # Replace with your directory path\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b6f2e6d-5498-4933-93e4-ffeeae78b2cb",
   "metadata": {},
   "source": [
    "## Considerations:"
   ]
  },
  {
   "cell_type": "raw",
   "id": "126789b5-6b2a-4e3d-b36d-3a4f9a53da67",
   "metadata": {},
   "source": [
    " Higher batch size did not improve it really"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51249299-a617-4c61-81f1-c9b001e73739",
   "metadata": {},
   "outputs": [],
   "source": [
    "Consider implementing gradient clipping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a60d171-7fd4-400f-99c8-3e752365f0ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "Consider implementing data augmentation techniques specific to audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "95917f91-b4dc-4ef0-8c0c-4fc3b3420646",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (272345262.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[3], line 1\u001b[1;36m\u001b[0m\n\u001b[1;33m    Consider: Adding type hints for better code maintenance\u001b[0m\n\u001b[1;37m                     ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "Consider: Adding type hints for better code maintenance\n",
    "Consider: Adding docstrings for better documentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0161a75-0617-4bbd-b7f2-55d5db1c8554",
   "metadata": {},
   "outputs": [],
   "source": [
    "Missing: Learning rate scheduling\n",
    "Missing: Early stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89b98a02-2957-4e35-9b8d-739cee4c3061",
   "metadata": {},
   "outputs": [],
   "source": [
    "Consider: Adding batch normalization layers\n",
    "Consider: Implementing dropout for better regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78691645-e77d-4ed1-ab01-6715fde35226",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9a47762-b64f-43d4-826c-fc31eb15e120",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "New guitar her env",
   "language": "python",
   "name": "guitar_hero"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
