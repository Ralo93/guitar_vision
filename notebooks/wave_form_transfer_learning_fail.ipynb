{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "11fb13cb-eb5f-44ee-b382-a4105af965cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VGGish model loaded successfully.\n",
      "VGGish model weights frozen.\n",
      "Input layer defined with variable length shape.\n",
      "Input processed through VGGish.\n",
      "Added classification layer.\n",
      "Keras model created successfully.\n",
      "Only the classification layer is trainable.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_29\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional_29\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                      </span>┃<span style=\"font-weight: bold\"> Output Shape             </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ waveform (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├───────────────────────────────────┼──────────────────────────┼───────────────┤\n",
       "│ lambda_30 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Lambda</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)              │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├───────────────────────────────────┼──────────────────────────┼───────────────┤\n",
       "│ logits (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">13</span>)               │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,677</span> │\n",
       "└───────────────────────────────────┴──────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape            \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ waveform (\u001b[38;5;33mInputLayer\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
       "├───────────────────────────────────┼──────────────────────────┼───────────────┤\n",
       "│ lambda_30 (\u001b[38;5;33mLambda\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)              │             \u001b[38;5;34m0\u001b[0m │\n",
       "├───────────────────────────────────┼──────────────────────────┼───────────────┤\n",
       "│ logits (\u001b[38;5;33mDense\u001b[0m)                    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m13\u001b[0m)               │         \u001b[38;5;34m1,677\u001b[0m │\n",
       "└───────────────────────────────────┴──────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,677</span> (6.55 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m1,677\u001b[0m (6.55 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,677</span> (6.55 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m1,677\u001b[0m (6.55 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[96], line 101\u001b[0m\n\u001b[0;32m     98\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m dataset\u001b[38;5;241m.\u001b[39mbatch(\u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m    100\u001b[0m \u001b[38;5;66;03m# Step 10: Train the model on mock data\u001b[39;00m\n\u001b[1;32m--> 101\u001b[0m train_dataset \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_dataset_from_directory\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m../data/raw/\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    103\u001b[0m model\u001b[38;5;241m.\u001b[39mcompile(optimizer\u001b[38;5;241m=\u001b[39mtf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39moptimizers\u001b[38;5;241m.\u001b[39mAdam(),\n\u001b[0;32m    104\u001b[0m               loss\u001b[38;5;241m=\u001b[39mtf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mlosses\u001b[38;5;241m.\u001b[39mSparseCategoricalCrossentropy(),\n\u001b[0;32m    105\u001b[0m               metrics\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m    107\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining model...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[96], line 91\u001b[0m, in \u001b[0;36mcreate_dataset_from_directory\u001b[1;34m(directory, sample_rate)\u001b[0m\n\u001b[0;32m     89\u001b[0m file_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(directory, file_name)\n\u001b[0;32m     90\u001b[0m audio \u001b[38;5;241m=\u001b[39m load_audio_file(file_path, sample_rate)\n\u001b[1;32m---> 91\u001b[0m label \u001b[38;5;241m=\u001b[39m \u001b[43mget_label_from_filename\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     93\u001b[0m audio_files\u001b[38;5;241m.\u001b[39mappend(audio)\n\u001b[0;32m     94\u001b[0m labels\u001b[38;5;241m.\u001b[39mappend(label)\n",
      "Cell \u001b[1;32mIn[96], line 81\u001b[0m, in \u001b[0;36mget_label_from_filename\u001b[1;34m(filename)\u001b[0m\n\u001b[0;32m     80\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_label_from_filename\u001b[39m(filename):\n\u001b[1;32m---> 81\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mint\u001b[39m(\u001b[43mfilename\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m_\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;241m0\u001b[39m])\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import soundfile as sf\n",
    "import librosa\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "\n",
    "# Step 1: Load the VGGish model from TensorFlow Hub\n",
    "vggish_model = hub.load('https://tfhub.dev/google/vggish/1')\n",
    "print(\"VGGish model loaded successfully.\")\n",
    "\n",
    "# Freezing the VGGish model weights\n",
    "vggish_model.trainable = False\n",
    "print(\"VGGish model weights frozen.\")\n",
    "\n",
    "# Step 2: Define the input shape for raw waveforms (variable length audio)\n",
    "input_waveform = tf.keras.Input(shape=(None,), dtype=tf.float32, name='waveform')\n",
    "print(\"Input layer defined with variable length shape.\")\n",
    "\n",
    "def process_vggish(waveform):\n",
    "    sample_rate = 16000\n",
    "    chunk_size = 15360  # 0.96s of audio = 15360 samples at 16kHz\n",
    "    \n",
    "    # Pad the waveform to ensure it can be divided into 0.96s chunks\n",
    "    num_samples = tf.shape(waveform)[-1]\n",
    "    padding_needed = chunk_size - (num_samples % chunk_size)\n",
    "    waveform_padded = tf.pad(waveform, [[0, padding_needed]])\n",
    "    \n",
    "    # Reshape the padded waveform to chunks of 0.96s (15360 samples)\n",
    "    waveform_chunks = tf.reshape(waveform_padded, [-1, chunk_size])\n",
    "    \n",
    "    # Get the embeddings from VGGish\n",
    "    embeddings = vggish_model(waveform_chunks)\n",
    "    \n",
    "    # Average the embeddings if there are multiple chunks\n",
    "    embedding_mean = tf.reduce_mean(embeddings, axis=0)\n",
    "    print(embedding_mean)\n",
    "    return embedding_mean\n",
    "\n",
    "# Step 4: Use a Lambda layer to incorporate the VGGish processing\n",
    "vggish_output = tf.keras.layers.Lambda(process_vggish, output_shape=(128,))(input_waveform)\n",
    "print(\"Input processed through VGGish.\")\n",
    "\n",
    "# Step 5: Add a classification layer\n",
    "num_classes = 13  # Example number of classes\n",
    "logits = tf.keras.layers.Dense(num_classes, activation='softmax', name='logits')(vggish_output)\n",
    "print(\"Added classification layer.\")\n",
    "\n",
    "# Step 6: Create the Keras Model\n",
    "model = tf.keras.Model(inputs=input_waveform, outputs=logits)\n",
    "print(\"Keras model created successfully.\")\n",
    "\n",
    "# Ensure the new classification layer is trainable\n",
    "for layer in model.layers:\n",
    "    if layer.name == 'logits':\n",
    "        layer.trainable = True\n",
    "    else:\n",
    "        layer.trainable = False\n",
    "print(\"Only the classification layer is trainable.\")\n",
    "\n",
    "# Step 7: View the model summary\n",
    "model.summary()\n",
    "\n",
    "# Step 9: Load the mock .wav files and prepare a TensorFlow Dataset\n",
    "def load_audio_file(file_path, sample_rate=16000):\n",
    "    audio, sr = librosa.load(file_path, sr=sample_rate)\n",
    "    return audio\n",
    "\n",
    "def get_label_from_filename(filename):\n",
    "    return int(filename.split('_')[1].split('.')[0])  # Assuming filename format \"chord_X.wav\"\n",
    "\n",
    "def create_dataset_from_directory(directory, sample_rate=16000):\n",
    "    audio_files = []\n",
    "    labels = []\n",
    "    \n",
    "    for file_name in os.listdir(directory):\n",
    "        if file_name.endswith('.wav'):\n",
    "            file_path = os.path.join(directory, file_name)\n",
    "            audio = load_audio_file(file_path, sample_rate)\n",
    "            label = get_label_from_filename(file_name)\n",
    "            \n",
    "            audio_files.append(audio)\n",
    "            labels.append(label)\n",
    "    \n",
    "    # Convert to TensorFlow Dataset\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((np.array(audio_files), np.array(labels)))\n",
    "    return dataset.batch(2)\n",
    "\n",
    "# Step 10: Train the model on mock data\n",
    "train_dataset = create_dataset_from_directory(r'../data/raw/')\n",
    "\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(),\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "print(\"Training model...\")\n",
    "model.fit(train_dataset, epochs=5)\n",
    "print(\"Model trained successfully.\")\n",
    "\n",
    "# Step 11: Save the model\n",
    "save_path = './saved_model/vggish_classifier'\n",
    "model.save(save_path)\n",
    "print(f\"Model saved successfully to {save_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be6a1458-0aad-447e-902a-0b17082e0941",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "New guitar her env",
   "language": "python",
   "name": "guitar_hero"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
